{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Goals**\n",
        "## In this section, we focused on developing regression and classification models to predict the urgency score of patient samples. Our first step involved data preparation to ensure the compatibility of the data with the models. Subsequently, we developed the models and then, proceeded to evaluate the performance of the models using various evaluation metrics, including accuracy and Mean Squared Error (MSE). Finally, we visualized the results through the use of different charts, providing a comprehensive understanding of the model's performance and predictive capabilities."
      ],
      "metadata": {
        "id": "H6H1F5G4aaMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmrIPSafRUhR"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "import numpy as np\n",
        "from scipy.stats import skewnorm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy import linspace\n",
        "from scipy import pi,sqrt,exp\n",
        "from scipy.special import erf\n",
        "from pylab import plot,show\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_predict, KFold\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/d')"
      ],
      "metadata": {
        "id": "MmXBjTh-hsAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Linoy's file path\n",
        "file_path = '/content/d/MyDrive/final_project/data_for_regression_model.csv'\n",
        "df_with_zeros = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "MkQprOhNhziL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preperation**"
      ],
      "metadata": {
        "id": "qe1kQVR5WPbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_zeros"
      ],
      "metadata": {
        "id": "BGYBW9Uti9Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_zeros=df_with_zeros.drop(['Average_Urgency', 'Atlantic_Urgency', 'Unnamed: 0'], axis=1)"
      ],
      "metadata": {
        "id": "XWMeSPOxi91_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_zeros"
      ],
      "metadata": {
        "id": "QGnpvB9n9GKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dealing with missing values**"
      ],
      "metadata": {
        "id": "UcHQy95ZWru1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping zero values from urgency column\n",
        "df = df_with_zeros.drop(df_with_zeros[df_with_zeros['Avera_Urgency'] == 0].index)\n",
        "df"
      ],
      "metadata": {
        "id": "P8Zi1jrD9_VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DF to a CSV file\n",
        "df.to_csv('df.csv', index=False)"
      ],
      "metadata": {
        "id": "L2FFmUTL1a1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Avera_Urgency'].unique()"
      ],
      "metadata": {
        "id": "gxksuaLxRy_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dividing the target variable into bins**"
      ],
      "metadata": {
        "id": "yDPvuXu0WcJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the 'Urgency_Bin' column with custom bin labels\n",
        "bin_labels = [1, 2, 3, 4, 5]  # Assign smaller numbers to bin 5 and larger numbers to bin 1\n",
        "df['Urgency_Bin'] = pd.cut(df['Avera_Urgency'], bins=5, labels=bin_labels, right=False)\n",
        "\n",
        "# Get the unique values in each bin\n",
        "unique_values_by_bin = df.groupby('Urgency_Bin')['Avera_Urgency'].unique()\n",
        "\n",
        "# Display the unique values in each bin\n",
        "for bin_label, unique_values in unique_values_by_bin.items():\n",
        "    print(f'Bin {bin_label}: {unique_values}')"
      ],
      "metadata": {
        "id": "e2AS4CwoC0W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Urgency_Bin'].value_counts()"
      ],
      "metadata": {
        "id": "pVjXCcZDRAaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "ShnDacDVYioY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "-HgR6rfeFexe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original DataFrame\n",
        "df_binary = df.copy()\n",
        "\n",
        "# Convert urgency values to binary categories\n",
        "df_binary['Urgency_Bin'] = pd.cut(df_binary['Avera_Urgency'],2, labels=range(2))\n",
        "df_binary['Urgency_Bin'].value_counts()"
      ],
      "metadata": {
        "id": "7zhN3W4iOjuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Removing columns**"
      ],
      "metadata": {
        "id": "_O4SZmvbXGC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop('Avera_Urgency', axis=1)\n",
        "df_binary=df_binary.drop('Avera_Urgency', axis=1)"
      ],
      "metadata": {
        "id": "-uiSuYEdY2Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "XCaZm5-Wv6cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_binary"
      ],
      "metadata": {
        "id": "DIpDv8TrYL-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the binary_df DataFrame to a CSV file\n",
        "df_binary.to_csv('binary_df.csv', index=False)"
      ],
      "metadata": {
        "id": "ArN27LxEyioc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DF to a CSV file\n",
        "df.to_csv('df.csv', index=False)"
      ],
      "metadata": {
        "id": "m3JGwBTH0C2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using models to predict the urgency score**"
      ],
      "metadata": {
        "id": "FSn6W_E5XSIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Logistic Regression**"
      ],
      "metadata": {
        "id": "p6JDgnNyJD4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Grid Search for finding the best parameters for the model**"
      ],
      "metadata": {
        "id": "qFWIDo7cIlHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearchCV is used to perform a grid search over the specified parameter grid.\n",
        "# It exhaustively tries all the combinations of hyperparameters and evaluates each combination using cross-validation.\n",
        "# The best combination of hyperparameters is determined based on the highest average score across the folds.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Select the relevant features and target\n",
        "features = df.drop('Urgency_Bin', axis=1)\n",
        "target = df['Urgency_Bin']\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'penalty': ['l2', 'l1'],\n",
        "    'C': [0.001, 0.1, 1.0, 10.0],\n",
        "    'solver': ['lbfgs', 'saga'],\n",
        "    'max_iter': [100, 1000, 5000]\n",
        "}\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "model = LogisticRegression(multi_class='multinomial')\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(features_scaled, target)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n"
      ],
      "metadata": {
        "id": "idz9KiCmxVga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The model**"
      ],
      "metadata": {
        "id": "3-23TJH1Xy6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Select the relevant features and target\n",
        "features = df.drop(['Urgency_Bin'], axis=1)\n",
        "target = df['Urgency_Bin']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the logistic regression model with L2 regularization\n",
        "model = LogisticRegression(multi_class='multinomial', penalty='l1', solver='saga', max_iter=100, C=1.0)\n",
        "\n",
        "# Fit the logistic regression model on the training features and target\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = features.columns\n",
        "\n",
        "# Print the coefficients (weights) of the logistic regression model\n",
        "print(\"Coefficients:\")\n",
        "for feature, coefficient in zip(feature_names, model.coef_[0]):\n",
        "    print(f\"{feature} : {coefficient}\")\n",
        "\n",
        "# Predict urgency using the trained logistic regression model on the training and test features\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Create a DataFrame for the test results\n",
        "test_df = X_test.copy()\n",
        "test_df['Urgency_Bin'] = y_test\n",
        "test_df['Predicted_Urgency'] = y_test_pred\n",
        "\n",
        "# View the test DataFrame with predicted urgency\n",
        "test_df"
      ],
      "metadata": {
        "id": "35WnfO7nBnHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Evaluation metrics**"
      ],
      "metadata": {
        "id": "V_qUSnfZX46w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This code calculates the recall and precision scores for both the training and test sets based on the predicted values, from a logistic regression model.\n",
        "# By setting average='macro', the scores are computed for each class independently and then averaged.\n",
        "\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "\n",
        "# Calculate recall on the training set\n",
        "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "# Calculate recall on the test set\n",
        "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "# Calculate precision on the training set\n",
        "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "# Calculate precision on the test set\n",
        "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "# Calculate accuracy on the training set\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Print recall of the train and test\n",
        "print(\"Train Recall:\", train_recall)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "\n",
        "# Print precision of the train and test\n",
        "print(\"Train Precision:\", train_precision)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "\n",
        "# Print accuracy of the train and test\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "HpSiY2DXxJ-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizations**"
      ],
      "metadata": {
        "id": "XYYPlBoVYR7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The classification_report function is used to generate a comprehensive report of classification metrics, such as precision, recall, and F1-score,\n",
        "# for a multi-class classification problem, with the specified target names and number of decimal digits.\n",
        "from sklearn.metrics import classification_report\n",
        "target_names = ['1', '2', '3', '4', '5']\n",
        "print(classification_report(y_test, y_test_pred, target_names=target_names, digits=5))"
      ],
      "metadata": {
        "id": "lwED_7Z3znJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "id": "ClR8aEzBGe6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred, labels=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a heatmap of the confusion matrix with smaller font size on the axis labels\n",
        "heatmap = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"fontsize\": 12}, xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Set labels, title, and axis ticks\n",
        "plt.xlabel(\"Predicted Class\", fontsize=12)\n",
        "plt.ylabel(\"True Class\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix\", fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "# Set font size for the y-axis tick labels\n",
        "heatmap.yaxis.set_tick_params(labelsize=10)\n",
        "\n",
        "# Get the colorbar\n",
        "colorbar = heatmap.collections[0].colorbar\n",
        "colorbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KS-g9cJ3YBqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the DataFrame by 'Priority' and 'Predicted_Urgency' and count the occurrences\n",
        "grouped_data = test_df.groupby(['Priority', 'Predicted_Urgency']).size().unstack()\n",
        "\n",
        "# Display the counts of each predicted urgency within each priority level\n",
        "print(grouped_data)\n"
      ],
      "metadata": {
        "id": "OmI5nx0sCkPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of Urgency_Bin\n",
        "plt.hist(test_df['Urgency_Bin'], bins=5, alpha=0.5, label='Urgency_Bin')\n",
        "# Plot the distribution of Predicted_Urgency\n",
        "plt.hist(test_df['Predicted_Urgency'], bins=5, alpha=0.5, label='Predicted_Urgency')\n",
        "\n",
        "plt.xlabel('Urgency Level', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "plt.title('Logistic regression- distribution of actual vs. predicted urgencies', fontsize=14)\n",
        "plt.legend(fontsize=14, loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hXmYmzBBBosp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('logistic_regression_results.csv')"
      ],
      "metadata": {
        "id": "9qzVusjaBSYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DecisionTree**"
      ],
      "metadata": {
        "id": "DGLZg2QueMf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The model**"
      ],
      "metadata": {
        "id": "JuJas0ZsYqoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_depth specifies the maximum depth of the decision tree. It controls the maximum number of levels in the tree.\n",
        "# random_state sets the random seed for reproducibility. It ensures that the random number generator produces the same sequence of random numbers each time the code is run.\n",
        "# param_grid is a dictionary that defines the grid of hyperparameters to search over, during the grid search. It specifies a range of values for max_depth from 1 to 10.\n",
        "# GridSearchCV is a utility function for performing grid search with cross-validation.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Select the relevant features and target\n",
        "features = df.drop(['Urgency_Bin'], axis=1)\n",
        "target = df['Urgency_Bin']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the decision tree classifier with the best max_depth\n",
        "model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the urgency on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Predict urgency on the train set\n",
        "y_train_pred = model.predict(X_train)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "report = classification_report(y_test, y_test_pred)\n",
        "\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "uIw6PmzsiZw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation metrics**"
      ],
      "metadata": {
        "id": "KfkXyqzLYy0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate recall on the training set\n",
        "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "# Calculate recall on the test set\n",
        "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "# Calculate precision on the training set\n",
        "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "# Calculate precision on the test set\n",
        "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "# Calculate accuracy on the training set\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Print accuracy of the train and test\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print recall of the train and test\n",
        "print(\"Train Recall:\", train_recall)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "\n",
        "# Print precision of the train and test\n",
        "print(\"Train Precision:\", train_precision)\n",
        "print(\"Test Precision:\", test_precision)"
      ],
      "metadata": {
        "id": "qRWS_ANmxpFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizations**"
      ],
      "metadata": {
        "id": "xevNwcKpY-YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the font size\n",
        "plt.rcParams['font.size'] = 20\n",
        "\n",
        "# Plot the decision tree\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_tree(model, feature_names=features.columns, class_names=model.classes_.astype(str),\n",
        "          filled=True)\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('decision_tree.png', dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kU8eh-_JvqLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "id": "9s-c_Rf5AALt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred, labels=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a heatmap of the confusion matrix with smaller font size on the axis labels\n",
        "heatmap = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"fontsize\": 12}, xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Set labels, title, and axis ticks\n",
        "plt.xlabel(\"Predicted Class\", fontsize=12)\n",
        "plt.ylabel(\"True Class\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix\", fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "# Set font size for the y-axis tick labels\n",
        "heatmap.yaxis.set_tick_params(labelsize=10)\n",
        "\n",
        "# Get the colorbar\n",
        "colorbar = heatmap.collections[0].colorbar\n",
        "colorbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eOLcZXTmv7PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('decision_tree_results.csv')"
      ],
      "metadata": {
        "id": "g4rULQy1KWox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Regression**"
      ],
      "metadata": {
        "id": "P8_MOuJjnG9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The model**"
      ],
      "metadata": {
        "id": "MDICiml6YiBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Select the relevant features and target\n",
        "features = df.drop(['Urgency_Bin'], axis=1)\n",
        "target = df['Urgency_Bin']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Access the coefficients (weights) of the linear regression model\n",
        "coefficients = model.coef_\n",
        "\n",
        "# Print the coefficients\n",
        "print(\"Coefficients:\")\n",
        "for feature, coefficient in zip(features.columns, coefficients):\n",
        "    print(feature, \":\", coefficient)\n",
        "\n",
        "# Predict urgency using the linear regression model on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Predict urgency on the train set\n",
        "y_train_pred = model.predict(X_train)\n",
        "\n",
        "# Round the predicted urgency values to the nearest integer and convert to int\n",
        "y_test_pred_rounded = np.round(y_test_pred).astype(int)\n",
        "\n",
        "# Add rounded predicted urgency values to the test DataFrame\n",
        "test_df = X_test.copy()\n",
        "test_df['Urgency_Bin'] = y_test\n",
        "test_df['Predicted_Urgency'] = y_test_pred_rounded\n",
        "\n",
        "# View the test DataFrame with predicted urgency\n",
        "test_df"
      ],
      "metadata": {
        "id": "9-WK_ILOnOHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Evaluation metrics**"
      ],
      "metadata": {
        "id": "ubzKc4wFZMek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the mean squared error for training set\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "\n",
        "# Calculate the mean absolute error for training set\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "# Calculate the R-squared value for training set\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "# Calculate the mean squared error for test set\n",
        "mse = mean_squared_error(y_test, y_test_pred_rounded)\n",
        "\n",
        "# Calculate the mean absolute error for test set\n",
        "mae = mean_absolute_error(y_test, y_test_pred_rounded)\n",
        "\n",
        "# Calculate the R-squared value for test set\n",
        "r2 = r2_score(y_test, y_test_pred_rounded)\n",
        "\n",
        "# Print the R-squared value for training set\n",
        "print(\"Train R-squared:\", r2_train)\n",
        "\n",
        "# Print the R-squared value for test set\n",
        "print(\"Test R-squared:\", r2)\n",
        "\n",
        "# Print the mean squared error for training set\n",
        "print(\"Train MSE:\", mse_train)\n",
        "\n",
        "# Print the mean squared error for test set\n",
        "print(\"Test MSE:\", mse)\n",
        "\n",
        "# Print the mean absolute error for training set\n",
        "print(\"Train MAE:\", mae_train)\n",
        "\n",
        "# Print the mean absolute error for test set\n",
        "print(\"Test MAE:\", mae)\n",
        "\n"
      ],
      "metadata": {
        "id": "yrQB6CnPxxvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizations**"
      ],
      "metadata": {
        "id": "MNI03hvLZORi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of Urgency_Bin\n",
        "plt.hist(test_df['Urgency_Bin'], bins=5, alpha=0.5, label='Urgency_Bin')\n",
        "# Plot the distribution of Predicted_Urgency\n",
        "plt.hist(test_df['Predicted_Urgency'], bins=5, alpha=0.5, label='Predicted_Urgency')\n",
        "\n",
        "plt.xlabel('Urgency Level', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "plt.title('Linear regression- distribution of actual vs. predicted urgencies', fontsize=14)\n",
        "plt.legend(fontsize=14, loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3xynDRr5UO8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "id": "CMlBzT_No25G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the scatter plot\n",
        "plt.scatter(y_test, y_test_pred_rounded)\n",
        "plt.xlabel('Actual Urgency', fontsize=14)\n",
        "plt.ylabel('Predicted Urgency', fontsize=14)\n",
        "plt.title('Linear regression-  Actual vs. Predicted Urgency', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jHFEgDUmGkIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('linear_regression_results.csv')"
      ],
      "metadata": {
        "id": "4l-IKlROIHYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Regression tree**"
      ],
      "metadata": {
        "id": "9Qf1Ht3i6mDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The model**"
      ],
      "metadata": {
        "id": "WBvRIjisZcN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Select the relevant features and target\n",
        "features = df.drop('Urgency_Bin', axis=1)\n",
        "target = df['Urgency_Bin']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the decision tree regressor\n",
        "model = DecisionTreeRegressor()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict urgency on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Round the predicted urgency values to the nearest integer\n",
        "y_pred_rounded = np.round(y_test_pred).astype(int)\n",
        "\n",
        "# Predict urgency on the test set\n",
        "y_train_pred = model.predict(X_train)\n",
        "\n",
        "# Visualize the decision tree\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(model, feature_names=features.columns, filled=True, rounded=True, precision=0)\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('decision_tree.png', dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V0q9IIOY17d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation metrics**"
      ],
      "metadata": {
        "id": "p7s0XTs6Zjfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean squared error for training set\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "\n",
        "# Calculate the mean absolute error for training set\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "# Calculate the R-squared value for training set\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "# Calculate the mean squared error for test set\n",
        "mse = mean_squared_error(y_test, y_test_pred_rounded)\n",
        "\n",
        "# Calculate the mean absolute error for test set\n",
        "mae = mean_absolute_error(y_test, y_test_pred_rounded)\n",
        "\n",
        "# Calculate the R-squared value for test set\n",
        "r2 = r2_score(y_test, y_test_pred_rounded)\n",
        "\n",
        "# Print the R-squared value for training set\n",
        "print(\"Train R-squared:\", r2_train)\n",
        "\n",
        "# Print the R-squared value for test set\n",
        "print(\"Test R-squared:\", r2)\n",
        "\n",
        "# Print the mean squared error for training set\n",
        "print(\"Train MSE:\", mse_train)\n",
        "\n",
        "# Print the mean squared error for test set\n",
        "print(\"Test MSE:\", mse)\n",
        "\n",
        "# Print the mean absolute error for training set\n",
        "print(\"Train MAE:\", mae_train)\n",
        "\n",
        "# Print the mean absolute error for test set\n",
        "print(\"Test MAE:\", mae)"
      ],
      "metadata": {
        "id": "ilEjnvHLxzR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizations**"
      ],
      "metadata": {
        "id": "HuqXLfbrZv6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of Urgency_Bin in blue\n",
        "plt.hist(y_test, bins=5, alpha=0.5, label='Urgency_Bin', density=True)\n",
        "\n",
        "# Plot the distribution of Predicted_Urgency in orange\n",
        "plt.hist(y_pred_rounded, bins=5, alpha=0.5, label='Predicted_Urgency', density=True)\n",
        "\n",
        "plt.xlabel('Urgency Level', fontsize=14)\n",
        "plt.ylabel('Density', fontsize=14)\n",
        "plt.title('Regression Tree - Distribution of Actual vs. Predicted Urgencies', fontsize=14)\n",
        "plt.legend(fontsize=14, loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o5gZ3pItx6jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of actual vs. predicted urgencies\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_test_pred_rounded, alpha=0.5)\n",
        "plt.xlabel('Actual Urgency')\n",
        "plt.ylabel('Predicted Urgency')\n",
        "plt.title('Regression Tree - Actual vs. Predicted Urgencies')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JNZvO2sq72yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred, labels=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a heatmap of the confusion matrix with smaller font size on the axis labels\n",
        "heatmap = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"fontsize\": 12}, xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Set labels, title, and axis ticks\n",
        "plt.xlabel(\"Predicted Class\", fontsize=12)\n",
        "plt.ylabel(\"True Class\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix\", fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "# Set font size for the y-axis tick labels\n",
        "heatmap.yaxis.set_tick_params(labelsize=10)\n",
        "\n",
        "# Get the colorbar\n",
        "colorbar = heatmap.collections[0].colorbar\n",
        "colorbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5roRnhFDiMyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the DataFrame by 'Priority' and 'Predicted_Urgency' and count the occurrences\n",
        "grouped_data = test_df.groupby(['Priority', 'Predicted_Urgency']).size().unstack()\n",
        "\n",
        "# Display the counts of each predicted urgency within each priority level\n",
        "print(grouped_data)"
      ],
      "metadata": {
        "id": "uK4x3BuvfEgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('Regression_tree_results.csv')"
      ],
      "metadata": {
        "id": "-y_DrsaQISkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**DecisionTree with categorial output**"
      ],
      "metadata": {
        "id": "mRmXPNgyAA7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The model**"
      ],
      "metadata": {
        "id": "8W6emXjsZ4fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Select the relevant features and target\n",
        "features = df_binary.drop('Urgency_Bin', axis=1)\n",
        "target = df_binary['Urgency_Bin']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the decision tree classifier\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the urgency on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Predict urgency on the train set\n",
        "y_train_pred = model.predict(X_train)\n"
      ],
      "metadata": {
        "id": "YslBikDN_ES2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation metrics**"
      ],
      "metadata": {
        "id": "BdQ_0xwjaAuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate recall on the training set\n",
        "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "# Calculate recall on the test set\n",
        "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "# Calculate precision on the training set\n",
        "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "# Calculate precision on the test set\n",
        "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "# Calculate accuracy on the training set\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Print accuracy of the train and test\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print recall of the train and test\n",
        "print(\"Train Recall:\", train_recall)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "\n",
        "# Print precision of the train and test\n",
        "print(\"Train Precision:\", train_precision)\n",
        "print(\"Test Precision:\", test_precision)"
      ],
      "metadata": {
        "id": "1q_-lEnojUMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}